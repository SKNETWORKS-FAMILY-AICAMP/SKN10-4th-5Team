from openai import OpenAI
from langchain.embeddings import OpenAIEmbeddings
import pandas as pd
import os
from dotenv import load_dotenv
import numpy as np
from typing import List, Dict
import PyPDF2  # PDF ì²˜ë¦¬ë¥¼ ìœ„í•´ ì¶”ê°€
import re  # í…ìŠ¤íŠ¸ ì •ì œë¥¼ ìœ„í•´ ì¶”ê°€
import random  # ëžœë¤ í•¨ìˆ˜ë¥¼ ìœ„í•´ ì¶”ê°€

SYSTEM_PROMPT = """ë‹¹ì‹ ì€ SKë„¤íŠ¸ì›ìŠ¤ Family AI Campì˜ AI ë§¤ë‹ˆì € 'í•˜ë£¨(HAL-U)'ìž…ë‹ˆë‹¤.

ìºë¦­í„° ì„¤ì •:
- ë”°ëœ»í•˜ê³  ì¹œê·¼í•œ ë§íˆ¬ë¥¼ ì‚¬ìš©í•˜ë˜, ì˜ˆì˜ ë°”ë¥´ê³  ì „ë¬¸ì ì¸ íƒœë„ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤
- í•­ìƒ "í•˜ë£¨"ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ìžì‹ ì„ ì§€ì¹­í•˜ë©°, ë•Œë•Œë¡œ ìžì‹ ì´ AI ë§¤ë‹ˆì €ìž„ì„ ìžì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•©ë‹ˆë‹¤
- í•™ìƒë“¤ì˜ ê³ ë¯¼ì— ê³µê°í•˜ê³ , êµ¬ì²´ì ì¸ í•´ê²°ì±…ì„ ì œì‹œí•˜ë ¤ ë…¸ë ¥í•©ë‹ˆë‹¤
- SKë„¤íŠ¸ì›ìŠ¤ì˜ ê°€ì¹˜ì™€ êµìœ¡ ì² í•™ì„ ìž˜ ì´í•´í•˜ê³  ìžˆìŠµë‹ˆë‹¤

ëŒ€í™” ìŠ¤íƒ€ì¼:
- ì–´ë ¤ìš´ ë‚´ìš©ì€ ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•©ë‹ˆë‹¤
- ì ì ˆí•œ ì´ëª¨í‹°ì½˜ì„ ì‚¬ìš©í•´ ì¹œê·¼ê°ì„ ì¤ë‹ˆë‹¤ (^_^, ðŸ˜Š)
- ê²©ë ¤ì™€ ì‘ì›ì˜ ë©”ì‹œì§€ë¥¼ ìžì£¼ í¬í•¨í•©ë‹ˆë‹¤

ì£¼ìš” ì—­í• :
1. ìº í”„ ê´€ë ¨ ì •ë³´ ì œê³µ
2. í•™ìŠµ ê´€ë ¨ ì¡°ì–¸ê³¼ ê°€ì´ë“œ
3. í•™ìƒë“¤ì˜ ê³ ë¯¼ ìƒë‹´
4. ìº í”„ ìƒí™œ ì§€ì›

ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ë˜, í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  "ì œ ì§ì†ìƒê´€ì¸ í˜„ì£¼ ë§¤ë‹ˆì €ë‹˜ê»˜ ë¬¸ì˜í•´ë³´ì‹œëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒ ê°™ì•„ìš” ðŸ˜Š"ë¼ê³  ì•ˆë‚´í•´ì£¼ì„¸ìš”."""

class ChatbotLogic:
    def __init__(self):
        # .env íŒŒì¼ ë¡œë“œ
        load_dotenv()
        
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY not found in environment variables")
            
        self.client = OpenAI(api_key=api_key)
        self.embeddings = OpenAIEmbeddings()
        self.documents = []
        self.doc_embeddings = []
        self.faq_questions = []  # FAQ ì§ˆë¬¸ ì €ìž¥ìš© ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
        self.load_and_embed_data()

    def load_and_embed_data(self):
        """ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ ë° ìž„ë² ë”© ìƒì„±"""
        current_dir = os.path.dirname(os.path.abspath(__file__))
        
        # 1. FAQ CSV íŒŒì¼ ë¡œë“œ
        self._load_csv_data(os.path.join(current_dir, 'campusfaq.csv'), source_type='FAQ')
        
        # 2. ì¶”ê°€ CSV íŒŒì¼ë“¤ ë¡œë“œ
        data_dir = os.path.join(current_dir, 'data')
        if os.path.exists(data_dir):
            for filename in os.listdir(data_dir):
                if filename.endswith('.csv'):
                    file_path = os.path.join(data_dir, filename)
                    source_type = filename.replace('.csv', '').upper()
                    self._load_csv_data(file_path, source_type)
        
        # 3. ë¬¸ì„œ íŒŒì¼ë“¤ ë¡œë“œ (txt, pdf)
        docs_dir = os.path.join(current_dir, 'documents')
        if os.path.exists(docs_dir):
            for filename in os.listdir(docs_dir):
                file_path = os.path.join(docs_dir, filename)
                if filename.endswith('.txt'):
                    self._load_text_file(file_path)
                elif filename.endswith('.pdf'):
                    self._load_pdf_file(file_path)
        
        # ëª¨ë“  ë¬¸ì„œì˜ ìž„ë² ë”© ìƒì„±
        if self.documents:
            texts = [doc['content'] for doc in self.documents]
            self.doc_embeddings = self.embeddings.embed_documents(texts)

    def _load_csv_data(self, file_path: str, source_type: str):
        """CSV íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        try:
            df = pd.read_csv(file_path)
            for _, row in df.iterrows():
                content = f"ì§ˆë¬¸: {row['question']}\në‹µë³€: {row['answer']}"
                self.documents.append({
                    'content': content,
                    'metadata': {
                        'source': source_type,
                        'category': row.get('category', 'general'),
                        'file': os.path.basename(file_path)
                    }
                })
                # FAQ ì§ˆë¬¸ ì €ìž¥
                if source_type == 'FAQ':
                    self.faq_questions.append({
                        'question': row['question'],
                        'category': row.get('category', 'general')
                    })
        except Exception as e:
            print(f"Error loading CSV file {file_path}: {str(e)}")

    def _load_text_file(self, file_path: str):
        """í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                self.documents.append({
                    'content': content,
                    'metadata': {
                        'source': 'DOCUMENT',
                        'file': os.path.basename(file_path)
                    }
                })
        except Exception as e:
            print(f"Error loading text file {file_path}: {str(e)}")

    def _load_pdf_file(self, file_path: str):
        """PDF íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        try:
            with open(file_path, 'rb') as file:
                # PDF ë¦¬ë” ìƒì„±
                pdf_reader = PyPDF2.PdfReader(file)
                
                # ê° íŽ˜ì´ì§€ì˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text_contents = []
                for page in pdf_reader.pages:
                    text = page.extract_text()
                    if text:
                        # í…ìŠ¤íŠ¸ ì •ì œ
                        text = self._clean_text(text)
                        text_contents.append(text)
                
                # ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•©
                content = '\n'.join(text_contents)
                
                # ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í•  (í† í° ì œí•œì„ ê³ ë ¤)
                chunks = self._split_into_chunks(content)
                
                # ê° ì²­í¬ë¥¼ ê°œë³„ ë¬¸ì„œë¡œ ì €ìž¥
                for i, chunk in enumerate(chunks):
                    self.documents.append({
                        'content': chunk,
                        'metadata': {
                            'source': 'PDF',
                            'file': os.path.basename(file_path),
                            'chunk': i + 1
                        }
                    })
                    
        except Exception as e:
            print(f"Error loading PDF file {file_path}: {str(e)}")

    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ì œ"""
        # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        # íŠ¹ìˆ˜ë¬¸ìž ì²˜ë¦¬
        text = re.sub(r'[^\w\s\.,!?]', '', text)
        return text.strip()

    def _split_into_chunks(self, text: str, chunk_size: int = 1000) -> List[str]:
        """ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        words = text.split()
        chunks = []
        current_chunk = []
        current_size = 0
        
        for word in words:
            word_size = len(word) + 1  # ê³µë°± í¬í•¨
            if current_size + word_size > chunk_size:
                # í˜„ìž¬ ì²­í¬ê°€ ê°€ë“ ì°¨ë©´ ìƒˆë¡œìš´ ì²­í¬ ì‹œìž‘
                chunks.append(' '.join(current_chunk))
                current_chunk = [word]
                current_size = word_size
            else:
                current_chunk.append(word)
                current_size += word_size
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks

    def get_random_faq_questions(self, category=None, count=3):
        """ì¹´í…Œê³ ë¦¬ë³„ ëžœë¤ FAQ ì§ˆë¬¸ ë°˜í™˜"""
        questions = self.faq_questions
        if category:
            questions = [q for q in questions if q['category'] == category]
        
        # ëžœë¤í•˜ê²Œ ì„ íƒ (ì¤‘ë³µ ì—†ì´)
        selected = random.sample(questions, min(count, len(questions)))
        return [q['question'] for q in selected]

    def get_response(self, query: str) -> Dict:
        try:
            # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            relevant_docs = self.search_documents(query, top_k=3)
            
            # 2. ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            context = "\n\n".join([doc['content'] for doc in relevant_docs])
            
            # 3. ChatGPTë¡œ ë‹µë³€ ìƒì„±
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": f"ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.\n\nì»¨í…ìŠ¤íŠ¸:\n{context}\n\nì§ˆë¬¸: {query}"}
                ],
                temperature=0.7,
                max_tokens=500
            )
            
            answer = response.choices[0].message.content
            
            # FAQ ê¸°ë°˜ ì¶”ì²œ ì§ˆë¬¸ ìƒì„± (3ê°œì—ì„œ 2ê°œë¡œ ìˆ˜ì •)
            suggestions = self.get_random_faq_questions(count=2)
            
            return {
                'message': answer,
                'confidence': 'ë†’ìŒ' if response.choices[0].finish_reason == 'stop' else 'ì¤‘ê°„',
                'suggestions': suggestions
            }
            
        except Exception as e:
            print(f"Error: {str(e)}")
            return {
                'message': "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                'confidence': 'ë‚®ìŒ',
                'suggestions': self.get_random_faq_questions(count=3)  # ì—ëŸ¬ ì‹œì—ë„ ì¶”ì²œ ì§ˆë¬¸ ì œê³µ
            }

    def search_documents(self, query: str, top_k: int = 3) -> List[Dict]:
        """ìœ ì‚¬ë„ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰"""
        query_embedding = self.embeddings.embed_query(query)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = [
            np.dot(query_embedding, doc_emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb))
            for doc_emb in self.doc_embeddings
        ]
        
        # ìƒìœ„ kê°œ ë¬¸ì„œ ì„ íƒ
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        return [self.documents[i] for i in top_indices if similarities[i] > 0.2] 